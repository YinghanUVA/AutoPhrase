{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "# hypers\n",
    "dataset_names = ['inspec', 'krapivin', 'semeval', 'nus']\n",
    "root_path = \"/zf18/yw9fm/KPG_Project\"\n",
    "data_path = os.path.join(root_path,\"data\")\n",
    "# DEC_MODEL = \"decode_model_500000_1587513120\"\n",
    "DEC_MODEL = \"decode_model_495000_1587512303\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyphras\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "doc = spacy_nlp(u'keyphrase generator')\n",
    "for token in doc:\n",
    "#     print(token.text)\n",
    "    print(' '.join([stemmer.stem(w) for w in token.text.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing functions\n",
    "\n",
    "# write test data and summary to a new .txt file\n",
    "def write_aug_test(name, test_data):\n",
    "    dec_dir = os.path.join(root_path, 'log', name, DEC_MODEL)\n",
    "    gen_dir = os.path.join(dec_dir,'rouge_dec_dir')\n",
    "    gen = os.listdir(gen_dir)\n",
    "    gen = sorted(gen)\n",
    "    target_txt_path = \"../data/EN/\"+name+\".test.100aug.txt\"\n",
    "    with open(target_txt_path,'w+') as f_in:\n",
    "        for idx, data in enumerate(test_data):\n",
    "            read_path = os.path.join(gen_dir,gen[idx])\n",
    "            assert os.path.exists(read_path)\n",
    "            with open(read_path,'r') as f_out:\n",
    "                tmp_sum = f_out.readlines()\n",
    "            summ = ' $$ '.join([tmp.strip() for tmp in tmp_sum])\n",
    "            abstract = data['abstract']\n",
    "            title = data['title']\n",
    "            art = '\\n'.join((title, summ, abstract)).strip()\n",
    "            f_in.write(art + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspec True 500\n",
      "krapivin True 460\n",
      "semeval True 100\n",
      "nus True 211\n"
     ]
    }
   ],
   "source": [
    "# write datasets to AutoPhrase framework\n",
    "for name in dataset_names:\n",
    "    test_path = os.path.join(data_path,name,name+'_test_spacynp.json')\n",
    "    test_data = [json.loads(line) for line in open(test_path, 'r')]\n",
    "    write_aug_test(name, test_data)\n",
    "    print(name,os.path.exists(test_path),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import math\n",
    "# this part should be processed after autophrase.sh and phrasal_segmentation.sh\n",
    "\n",
    "EVAL_K = [10, 50]\n",
    "\n",
    "first_k = [100]\n",
    "\n",
    "def prepare_aug(test_data, name,k ,model_name='kp20k'):\n",
    "    # extract ranking score\n",
    "#     print(\"extracting phrase quality scores\")\n",
    "    data_size = len(test_data)\n",
    "    m_weight = {}\n",
    "    s_weight = {}\n",
    "    a_weight = {}\n",
    "    multi_path = \"../models/\" + model_name +\"/AutoPhrase_multi-words.txt\"\n",
    "    single_path = \"../models/\" + model_name +\"/AutoPhrase_single-word.txt\"\n",
    "    auto_path = \"../models/\" + model_name +\"/AutoPhrase.txt\"\n",
    "    with open(multi_path, 'r') as m:\n",
    "        lines = [line.lower().rstrip().split('\\t') for line in m]\n",
    "    for score,key in lines:\n",
    "        m_weight[key] = float(score)\n",
    "    with open(single_path, 'r') as s:\n",
    "        lines = [line.lower().rstrip().split('\\t') for line in s]\n",
    "    for score,key in lines:\n",
    "        s_weight[key] = float(score)\n",
    "\n",
    "    with open(auto_path, 'r') as s:\n",
    "        lines = [line.lower().rstrip().split('\\t') for line in s]\n",
    "    for score,key in lines:\n",
    "        a_weight[key] = float(score)\n",
    "    \n",
    "    # extract phrase and evaluate original text dataset\n",
    "#     print(\"extracting phrases and calculate tf-idf\")\n",
    "    seg_path = \"../models/\" + model_name +\"/segmentation.\"+name+\".100aug.txt\"\n",
    "    \n",
    "    with open(seg_path,'r') as f:\n",
    "        lines = [line.lower().rstrip() for line in f]\n",
    "    assert len(lines) == 3*data_size\n",
    "    aug_tf = []\n",
    "    absent_aug_tf = []\n",
    "    phrase = set()\n",
    "    ref = []\n",
    "    for j in range(data_size):\n",
    "        title_k = re.findall('<phrase>(.+?)</phrase>', lines[3*j])\n",
    "        sum_k = ' '.join(lines[3*j+1].split('$$')[:k])\n",
    "        sum_k = re.findall('<phrase>(.+?)</phrase>', sum_k)\n",
    "        abstract_k = re.findall('<phrase>(.+?)</phrase>', lines[3*j+2])  \n",
    "        counts = dict()\n",
    "        count_absent = dict()\n",
    "        for i in title_k:\n",
    "            counts[i] = counts.get(i, 0) + 5\n",
    "            phrase.add(i)\n",
    "        for i in sum_k:\n",
    "            counts[i] = counts.get(i, 0) + 0.5/100\n",
    "            count_absent[i] = count_absent.get(i,0) + 1\n",
    "            phrase.add(i)\n",
    "        for i in abstract_k:\n",
    "            counts[i] = counts.get(i, 0) + 1  \n",
    "            phrase.add(i)\n",
    "        # update tf with phrase quality\n",
    "        for item in counts.keys():\n",
    "            if item in m_weight:\n",
    "                counts[item] = counts[item]*m_weight[item]\n",
    "            elif item in s_weight:\n",
    "                counts[item] = counts[item]*s_weight[item]*0.1\n",
    "            else:\n",
    "                counts[item] = counts[item]\n",
    "        aug_tf.append(counts)\n",
    "        absent_aug_tf.append(count_absent)\n",
    "    if os.path.exists(os.path.join(model_name,name+'_aug_df'+str(k)+'.json')):\n",
    "        aug_df = json.load(open(os.path.join(model_name,name+'_aug_df'+str(k)+'.json'),'r'))\n",
    "    else:\n",
    "        aug_df = {}\n",
    "        for idx,kp in enumerate(phrase):\n",
    "#             if idx % 1000 ==0:\n",
    "#                 print(idx)\n",
    "            for i in range(data_size):\n",
    "                if kp in aug_tf[i]:\n",
    "                    aug_df[kp] = aug_df.get(kp,0)+1\n",
    "        with open(os.path.join(model_name,name+'_aug_df'+str(k)+'.json'), 'w') as fp:\n",
    "            json.dump(aug_df, fp)\n",
    "    assert len(phrase)<=len(aug_df)\n",
    "        \n",
    "    result = []\n",
    "    for i in range(data_size):\n",
    "        term_freq = aug_tf[i]\n",
    "        score_dict = {}\n",
    "        for key, value in term_freq.items():\n",
    "            assert aug_df[key]>=1\n",
    "            score_dict[key] = value * math.log(data_size/aug_df[key])\n",
    "        result.append(sorted(score_dict, key=score_dict.get)[::-1])\n",
    "        \n",
    "    absent_result = []\n",
    "    for i in range(data_size):\n",
    "        term_freq = absent_aug_tf[i]\n",
    "        score_dict = {}\n",
    "        for key, value in term_freq.items():\n",
    "            assert aug_df[key]>=1\n",
    "            score_dict[key] = value * math.log(data_size/aug_df[key])\n",
    "        absent_result.append(sorted(score_dict, key=score_dict.get)[::-1])\n",
    "    \n",
    "#     precision,recall = evaluate_oracle(present,result)\n",
    "    print('Start Evaluating:')\n",
    "    for score in EVAL_K:\n",
    "        for gd in [absent]:\n",
    "            print('top_k:',score)\n",
    "            evaluate_recall(score,gd,absent_result)\n",
    "#             evaluate(score,gd,result)\n",
    "#     return result, precision, recall\n",
    "\n",
    "\n",
    "def evaluate_recall(K, gold, result):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    assert len(gold)==len(result)\n",
    "    for gt, rank in zip(gold, result):\n",
    "        if len(gt)>=1:\n",
    "            gt = [' '.join(ls) for ls in gt]\n",
    "            ref = set()\n",
    "            i = 0\n",
    "            while len(ref)<K and len(rank)>i:\n",
    "                ref.add(stem_process(rank[i]))\n",
    "                i += 1\n",
    "            tmp = 0\n",
    "            for rk in ref:\n",
    "                if rk in gt:\n",
    "                    tmp += 1\n",
    "            recall.append(tmp*1.0/len(gt))\n",
    "            precision.append(tmp*1.0/K)\n",
    "            \n",
    "    p = sum(precision)/len(precision)\n",
    "    r = sum(recall)/len(recall)\n",
    "    print('recall:',r)\n",
    "    if p > 0:\n",
    "        print('f1:',2*p*r/(p+r))\n",
    "def stem_process(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    return ' '.join([stemmer.stem(w) for w in doc.text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspec evlauate augmented data\n",
      "100\n",
      "Start Evaluating:\n",
      "top_k: 10\n",
      "recall: 0.00020242914979757087\n",
      "f1: 0.0002288329519450801\n",
      "top_k: 50\n",
      "recall: 0.018184885290148448\n",
      "f1: 0.0012207597096449074\n",
      "krapivin evlauate augmented data\n",
      "100\n",
      "Start Evaluating:\n",
      "top_k: 10\n",
      "recall: 0.001199040767386091\n",
      "f1: 0.0006851661527920521\n",
      "top_k: 50\n",
      "recall: 0.028617106314948036\n",
      "f1: 0.00247780396740865\n",
      "semeval evlauate augmented data\n",
      "100\n",
      "Start Evaluating:\n",
      "top_k: 10\n",
      "recall: 0.0\n",
      "top_k: 50\n",
      "recall: 0.007295173961840628\n",
      "f1: 0.0017745017745017745\n",
      "nus evlauate augmented data\n",
      "100\n",
      "Start Evaluating:\n",
      "top_k: 10\n",
      "recall: 0.002307692307692308\n",
      "f1: 0.0014201183431952664\n",
      "top_k: 50\n",
      "recall: 0.0282460506954434\n",
      "f1: 0.0045284401727506305\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "for name in dataset_names:\n",
    "    test_path = os.path.join(data_path,name,name+'_test_spacynp.json')\n",
    "    test_data = [json.loads(line) for line in open(test_path, 'r')]\n",
    "    # extract present/absent groundtruth\n",
    "    present = [[[stemmer.stem(word) for word in words] for words in data['present_tgt_phrases']] for data in test_data]\n",
    "    absent = [[[stemmer.stem(word) for word in words] for words in data['absent_tgt_phrases']] for data in test_data]\n",
    "      \n",
    "    \n",
    "#     print(name,'evaluate normal:')\n",
    "#     prepare_normal(test_data,name)\n",
    "    print(name,'evlauate augmented data')\n",
    "    for i in first_k:\n",
    "        print(i)\n",
    "        prepare_aug(test_data,name,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc.text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('msr_venv': conda)",
   "language": "python",
   "name": "python36964bitmsrvenvcondaf79bac95f32646f8a517b85c47172b55"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
